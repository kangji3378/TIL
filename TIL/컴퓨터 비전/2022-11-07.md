l1 regularization : 크기 x(항상 동일) 부호만
l2 regularization : 크기 0

regularization ppt 26p

Dropout: 확률적으로 가중치 중 몇개의 연결을 끊는다
    => 가중치를 다 쓰는 것보다 overfitting에 빠질 확률이 낮아진다
    => ensemble learning 중 하나
    => 가중치를 껐다 켰다하면서 어려모양의 모델을 학습됐기 때문에 ensemble 효과를 낼 수 있다

Early Stopping
    학습을 오래할 경우 overfitting 될 수 있으니깐 일찍 training를 멈춘다

Activation Finctions
    ReLU : f=W2 max(0,W1 x)



Backpropagation (Computational Graphs)
