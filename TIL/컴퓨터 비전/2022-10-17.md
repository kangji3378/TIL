# Deep learning
## supervised learning
feature들을 늘리는거= hand-crafted features
kullback-leibler divergence: p랑 q가 비슷할수록 줄어든다
=> cross entropy
H(p,q)=H(p)(이미 고정된 값이여서 빼든 더하든 상관없다)+D(p||q)
즉 cross entropy를 줄이는 것이 kullback-leibler divergence를 줄이는 거랑 같기 때문에 corss entropy를 사용한다.

공식 softmax 9p

note2
선형같은 경우 case를 나눌 필요가 없지만
multiclass classification같은 경우
w1 w2랑 연결= z1
w3 w4랑 연결= z2
근데 softmax를 할때 z1과 z2가 연결된다
그래서 i=j랑 i!=j 두가지를 생각해야한다(ex> 고양이벡터인것과 고양이벡터가 아닌 것)
역전파: 기여한 W를 업데이트

다시 softmax
공식 증명

Neural Network
층이 하나일때의 문제: 
* linear하지 않은 분류일 경우 학습 불가
    * solution1 : feature transform => habd craft feature
* ex> 정면, 측면의 물체를 모두 recognize하지 못한다
