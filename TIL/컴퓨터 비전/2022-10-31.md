# Regularization
## overfitting
* The model may fit the training data very well, but fails to generalize to new example (test or validation data)
## regularization
* 손실함수 조작(기준 추가(ex> loss+regulaizer))
* 10p
* Improve optimization by adding curvature
    * L(w)=L(w,y)+R(w)=> R(w)로 인해 loss-w 그래프의 모양이 변한다
* f=Wx(11p)
    * W가 크면 f는 x에 민감해진다(안좋다)
* L(W)=..+R(W) (12p)
    * R(W)=> W의 크기 라 했을때 R(w)가 줄어들면 L(W)또한 내려간다
        * W가 줄어든다
* L1 regularization
    * R(W) = |W|의 합
* L2 regularization
    * R(W) = root(W^2)의 합
* More complex
    * Dropout
    * Batch normalization
    * Cutout,Mixup,Stochastic depth..
* 13p

* 16p
    * f= inpout
    * 2번 이해(L2 reguraliztion)
* 21p
* 23p
    * L1 L2 미분값의 차이 L2 미분 확인
        * L1은 L2랑 다르게 w값과는 상관없이 0으로 미는 힘이 같다(25p)
