Gradient Descent Example

cost : L(w)
w*=arguminL(w)
wt+1<-wt-aL(w)

시작점(initial point)=x^(0)

Linear regression(실수 매핑 즉 분류랑 다름)?과의 차이 생각
Linear classification
3p
z=w^tx+b
쓰레쉬 홀드?
4p
파라미터를 줄이기위해
+b>=r -> b-r>=0

6p 예제
8p
9p
feasible region: linearly separable 해야지 찾을 수 있다

Logistic regression(얘는 classification, linear regression하고 다름)
classification에서도 Loss Function를 찾고 싶음
11p
0-1 loss=> 분류를 잘 했으면(y(예측)=t(정답)) 0, 아니면 1
=> 12p
14p
Loss function을 찾았지만 미분이 불가하므로 GD를 사용할 수 없다
=> 연속이아니라 discrete한 값이기 때문
    
    체인룰
        합성함수 미분법(공부 필요)
15p
classification을 regression으로 생각하기 위한 surrogate loss function
16p
17p =>문제점

range를 조절하기위해 생겨난
18p Logistic Activation function
sigmoid함수를 이용해서 y 범위를 조절해줌
sigmoid를 w^t*x=z에 씌우면 z가 0보다 크면 1, 0보다 작으면 0를 리턴한다
but gradient descent가 잘 동작하지 않음(19p)
z가 작은 값으로 가버리면 gradient가 0쪽으로 너무 가까워짐=gradient가 계산이 안된다

20p Activation function은 잠시 놓고 15p surrogate를 수정해본다
entropy= 정보량의 기댓값
cross entropy?
함수의 생김새=항아리모양처럼 생겨진다












