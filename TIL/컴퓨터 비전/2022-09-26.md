## Regression (회귀)
대표적인 Supervised Learning
regression == y값이 실수(ex> 연속적인 실수값, 몸무게)

## claasification(분류)
y is categorical == classification
x는 다차원일 수 있다
    (ex>암을 구별할 때 종앙의 크기뿐만 아니라 나이 또한 x일 수 있다)
    과거에는 특징 찾는 법에 대해 연구해야 했지만 최근에는 딥러닝이 특징까지도 찾아주기 때문에 중요도가 떨어졌다
supervised lerning(ex> 암의 양성 악성 구별, 개와 고양이)

# Regression
## Linear Functions
* x ∈ R^d(d차원만큼의 x) is called an input (a.k.a. features or covariates)
* B ∈ R^d is called the parameters (a.k.a. parameter vector)
* y = fB(x) is called the label (a.k.a. output or response)
6p i와  j의 차이 = i: 몇번째 데이터, j: 데이터 안에 몇번째 특징(x, feature)인지

## Choice of Loss Function (손실함수)
학습하고자 하는 모델의 파라미터의 기준=> 손실함수
즉 기준의 선택
Mean squared error (MSE)

중요한 점 : 어떻게 모델을 설계할 지와 어떤 손실함수를 선택하는 지, 데이터의 정제
            즉 조건과 설계를 잘해야한다
12p
arg min L(손실함수)의값을 최소로 만들어주는 파라미터
13p
B(베타)를 찾는법

20p
two design decitions!!