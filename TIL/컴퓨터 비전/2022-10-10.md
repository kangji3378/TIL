# Gradient of Logistic Loss
크로스 엔트로피
y= 모델

# Multi class
## Impurity
* 불순물 (1p)
    * 십자가: 불순물
3p 
P(X=i)=t(크로스엔트로피의 t(class분류))
H(x)
랜덤 변수의 정보량(log2P(X=i))의 기대값
==> 엔트로피

log2 왼쪽: t(실제 나온 값을 통해 나온 확률)
오른쪽; y( 모델에서 예측한 확률, 예측값들의 확률)

5p
Huffman code
확률이 큰 D가 length가 가장 짧다
그래서 비트수는 A=B>c>D

-log2p(x=i):비트수
p(x=i)=prob

틀리고 애매해지면 엔트로피의 수치(Loss Function)가 높아진다

11p
k개의 벡터차원에서 딱 하나의 k만 1개인 경우
one-hot-vetcot
ex>정답값(모델값x)t=(동물 10마리중 k번째 동물이 정답이다)(인덱스가 이미 정해져있다)
즉 y={0,0,0,1,0,0}이란 벡터가 된다
과거에는 결과값이 스칼라(0or1)이였다
그래서 z,b도 벡터가 된다
그럼 메트릭스와 벡터의 곱을 알아야한다

12p
그럼 결과는 어떻게 정해지는가?(과거에는 일정수치 이상 =1 미만=0)
=> 가장 큰 수치의 인덱스를 1, 나머지는 0으로 판별

=> softmax
: 모든 z1,z2,z3...에 softmax를 적용하면 모든 z의 합은 1이된다=> 정규화
yk=확률값이 된다
