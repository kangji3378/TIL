# Linear regression
9p
y=W^t*x
파란점 dataset D={(x1,t1),(x2,t2)..}
[w0 w1] [x0 x1](열)=w0x0+w1x1=y
y: 실수값
regression=연속적인 값
MSE=(yi(모델 예측값)-ti(실제 값))^2/n
gradient descent
 cost function(MSE)을 줄어야 한다
W,E(error)의 그래프 = 항아리 모양
W(t+1)=Wt-g*알파(런닝레이트)
# Linear classification
2p
* Binary 
* Multiclass
3p
z=w...
6p
not

13p
logisitic regression
24p 요런식
시그모이드

주구장창 29p

g=미분(L(cost function)/Wl)
Lce
체인룰

# multiclass classification
16p까지 설명했음


ppt에 없는것
D(x(사진),ti(고양이))
w가 메트릭스가 되어야 y값이 multiclass로 나온(ex>[0.2,0.2,0.8])
one hot vector
굵은 W(16p)
32*32*3의 사진=> 일렬로 펼치면 x=3072 ex로 W를 10*3072라 하면
W^t*x=10*1=>행렬값이 도출됨
bias는 x의 행렬의 마지막에 1를 추가하면 bias처리가 된다

Linear 특성 2가지
1. f(cx)=c*f(x)

영역연산 15p

19p
dot product
template matching(필터 한번만 매칭)

Not
linear classification
8p,9p seperate

신규 18p
XOR
20p
linear classification = perceptron coudn't learn XOR

22p 학습
Todo
23p
{(xi,yi)}
30p
softmax

CE= (-logy)정보량

까지error 계산

15p를 한것
16p를 알아야 descent를 할 수 있다
미분 (Lce/y*y/z*z/w)

신규
y=softmax
벡터[a]-> softmax[a]=[y]
softmax할때 모든 요소가 연계되므로 벡터안의 모든 요소를 고려해서 편미분해야한다

i=j
    ai=yj
i!=j
    ai!=yj
