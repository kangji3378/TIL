56p
Bias-variance Tradeoff
capacity가 점점 커질수록(feature가 늘어날수록) Training loss는 감소하지만, Test loss는 감소하다가 어느 capacity를 지나가면 증가한다(overfitting)
즉 Underfitting 과 overfitting 사이에 이상적인 Ideal한 capacity가 있다

but 아예 훈련데이터가 많으면 variance가 커져도 이상적인 모델을 만들 수 있다


Gradient Descent

gradient descent is an iterative(연산회수?) algorithm
edge detection은 gradient descent랑 다르다

cost function를 제일 낮게 만들어주는 w를 찾는 과정

gradient(가장 가파르게 증가시키는 방향이므로 gradient와 반대로 간다)
그래서 w를 편미분한 값에 마이너스를 붙인다
J= w에 대한 gradient? or w에 대한 `loss function`? or 편미분한 값? or j차원?
j번째 인덱스?
2p 적절한 w는 j(w)가 가장 낮은곳(이를 찾는 과정: gradient descent를 통해 w를 업데이트)

3p 직접해봐?
mean square error?

4p gradient descent의 장점

loss function vs training cost vs training loss

7p batch training=모든 dataset이용한다?

8p SGD= gradient descent 업데이트 인데 data 한개로도 가능하다
=> 원래 gradient descent는 전체 dataset를 써야하는데 SGD는 single training이 가능하다
=> 속도가 더 빠르지만 방향에 noise가 낀다 but stochastic은 unbiased estimate
=> 무작위로 한개만 뽑아서 계산하는 기댓값은 원래 하고자하는 gradient descent랑 동일하다(모집단에 대한 평균과 동일하다)

=> 한개만 쓰기 떄문에 varience 가 높아진다=> SGD에서 data를 추가해준다.(전체 데이터셋 batch 가 아닌 약간만 추가해준다 mini batch)
