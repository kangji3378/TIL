# 최적화
-제약조건이 있을 수도 있는 상황에서 함수의 최대치와 최소치를 찾는 것
46p
매개변수 공건에 전역최적해 뿐만 아니라 지역최적해가 있을 수 있다
## 기계학습 훈련 과정
* 목적함수 최적화를 위한 매개변수 공간 탐색
    * 학습모델의 매개변수 공간
        * 적절한 학습 모델 선택
        * 과업의 목적함수 정의
        * 학습 모델의 매개변수 공간을 탐색하여 목적함수가 최저가 되는 최적점을 찾는다
        * 47p참고

### 기계학습 최적화 전략
* 낱낱탐색 알고리즘
    * 하나씩 손실함수를 비교하는 방법=> 좋지 않은 방법
* 무작위탐색 알고리즘
    * 효율적이지 않은 방법
* 경사하강법 알고리즘
    * 미분: x=수, y=수
        * 입력에 따른 출력의 변화량
    * 49p
        * 방향(미분의 마이너스)은 알 수 있지만 얼만큼 이동해야하는가?
            * 하이퍼 파라미터
    * 경사도: x=벡터, y=수
        * 변수가 여러개인 함수의 미분=> 복수의 편미분
    * 자코비안,야코비언 행렬: x=벡터, y=벡터
    * 헤세 행렬: 2차 미분 행렬
### 미분의 연쇄법칙
* 합성함수의 미분
* 기계학습의 활용: 인공 신경망은 합성함수이므로 가중치의 경사도를 계산할 때 연쇄법칙 적용한다
    * 오류 역전파
* 경사하강법 52p 
    * 경사도를 활용하여 학습 모델의 최적 매개변수를 탐색하는 반복 최적화
        * batch 와 SGD or mini-batch 알고리즘이 있다
    * batch
        * 집단 경사 하강법 알고리즘
        * 모든 샘플을 탐색하기 때문에 오래걸린다
    * SGD 
        * 확률론적 경사 하강법
        1. 샘플의 모집단 중 하나의 데이터를 뽑고 이 데이터의 손실함수와 미분값을 구해서 갱신을 한다.
        2. 이 과정을 여러번 함으로서 대표성을 띄운다.
            * 53p Mometum: SGD에 관성(과거의 경험을 learning시켜)을 포함시켜 지역 최적해에 빠지지 않고 빠져나온다
    * mini-batch
        * 모집단 중 적은 수의 데이터들을 뽑고 이 데이터들을 이용해서 대표성을 띄운다
        * SGD보다 노이즈의 영향을 덜받으면서 batch보다 횟수가 적을 수 있다

# 정보 이론
* 확률 통계와 많은 교차점을 가진 정보이론은 기계학습의 기초적 근간을 제공한다
    * 확률통계: 확률 분포 추정, 생성모델 등
    * 정보이론: 불확실성을 정량화하여 정보이론을 기계학습에 활용 ex>교차 엔트로피 목적함수 등
        * 사건이 지닌 정보를 정량화
            * ex> 아침에 해가뜬다 < 아침에 일식이 있었다 (확률이 작을수록 많은 정보를 가지고 있다)
* 자기 정보 : 사건에 대한 정보량
    * ex> 동전에서 앞면이 나오는 사건의 정보량 -log2(1/2)=1
    * 주사위에서 1이 나오는 사건의 정보량 -log2(1/6)=2.58
        * => 후자의 사건이 전자의 사건보다 높은 정보량을 가진다
* 엔트로피: 사건들이 가지고 있는 확률변수x의 불확실성을 나타내는 엔트로피
    * 전체 사건 정보량의 기대값 (56p)
    * 57p 확률이 0이나 1일 경우 확실성으로 인해 정보량(엔트로피)가 0이다
    * 즉 불확실성이 가장 높은 경우(ex> 모든 사건이 동일한 확률을 가질 때) 엔트로피가 최고임

* KL발산(공식같은 거 시험에 나옴)
    * 두 확률분포 (P,Q)사이의 거리를 계산할 때 주로 사용
        * 근데 공식을 보면 대칭성은 아니다
* 교차 엔트로피 cross entropy: 두 확률분포 P,Q사이의 엔트로피
    * 60p H(P)는 고정값이므로 KL발산의 최소화시키는 방향으로 손실함수를 이용할 수 있다.
    시험에 나올만하다 61p
62p softmax를 통해 logits value를 확률변수로 바꾼다.
즉 예측값과 실제값(Ground Truth)은 확률분포가 되므로
cross entropy를 통해서 