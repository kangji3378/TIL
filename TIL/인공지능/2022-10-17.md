# 퍼셉트론
19p 
 -1: x가 음수인 영역
 1. 결국 -yk(wx)는 항상 양수다(loss function)
 2. 오답이 많아질수록 값이 커진다
 3. 오답이 하나도 없을 경우 0이다
(이러한 조건은 초기 퍼셉트론에서만의 조건이다)
최근=> 손실함수=0이어야한다는 조건은 만족하기 힘들다(19p y공집합일때)

20p
21p
gradient 벡터형태
갱신=> w->w+로우*g

23p

25p
퍼셉트론 학습
직접해보기
27p 다층 퍼셉트론
퍼셉트론은 선형분류기이기 때문에 선형분리가 불가능한 상황이 있다

다층 퍼셉트론의 핵심
* 은닉층: 특징 공간을 과업 수행에 유리한 새로운 특징 공간으로 변환
* 시그모이드 활성함수: 경성 출력의 계단 함수-> 연속적인 연성 출력의 시그모이드 함수
* 오류 역전파 학습 알고리즘: 복수층이 순차적으로 이어진 구조로 역방향 경사도를 한층 씩 계산하고 가중치를 갱신한다

29p
30p
특징 공간이 변경됨(x->z)
즉 새로운 특징 공간z에서 선형분리를 수행하는 퍼셉트론을 순차 결합할 수 있다
31p

활성함수 : 비선형
계단함수 : 미분불가
로지스틱 시그모이드 : 미분가능
하이퍼볼릭 탄젠트 시그모이드

선형연산 : 유사도 
이 유사도의 의미를 증폭하기 위해 활성함수를 사용

34p
시그모이드 는 미분이 0이기 때문에 gradient가 사라질 수 있다.
활성화가 됬을때는(입력에 변화요소가 생길 때) => 0이상 ReLU

35p
d차원 p차원 t차원
=> d차원에 비해 p차원은 분류가 쉬워진다. 이를 반복적으로 개선하다보면
선형으로도 분류할 수 있는 x차원이 나올 것이다.
(내적?)
36p

37p
U=> w
t=>활성함수
특징추출=특이공간변화

