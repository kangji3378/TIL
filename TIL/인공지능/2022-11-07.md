CNN
    Convolutional neural networks
완전 연결 => 부분연결(parameter의 수가 크게 줄어든다)

합성곱의 가중치 공유 (묶인 가중치)
    모든 노드가 동일한 kernel 사용

교차연관성 합성곱 차이가 있지만 통일함(교차연관성 그대로, 합성곱 뒤집기)

즉 합성곱은 내적을 부분적으로만 적용한다

24p
    유사도가 높다=> 학습시켰던 특징을 보유하고 있다

커널= 필터 = 공간필터(spatial filtering)

커널을 학습시킨다

합성곱이후 출력층(특징맵)의 크기 줄어들기 때문에 padding을 통해 크기를 유지한다
    zero padding (입력층에 0를 덧댄다)

26p 27p
    커널의 depth 와 입력의 depth는 맞춰야한다(30p)
    필터 수만큼 특징맵이 생긴다
    필터수가 클수록 좋다
        => 필터수가 적을수록 정보손실이 커진다
31p
다른커널을 하나의 입력에 적용해서 나온 특징맵들은 입력층의 depth가 된다

하이퍼 매개변수 : (깊이x), 높이, 폭, 보폭, 덧대기

padding: 계속해서 크기를 유지하는 것이 아니라 특정 레이어마다 컨볼루션의 보폭을 늘리면서 크기를 줄인다(특징 추상화(차원 감소))

선형연산 (컨볼루션) + 비선형연산
    (비선형연산의 기준을 0으로 맞춰야 하기 때문에 bias 추가) => but 완전히 0으로 맞추기는 어렵다

보폭: 보폭 키울시 출력 크기가 줄어들거나 입력의 중복성을 제거

35p 입력과 출력의 크기, 매개변수의 수 공식 중요

비선형연산

pooling (최근에는 안씀)
    중요치 않는 정보 삭제=> 사이즈 감소
    매개변수 x
    작은 변화에 둔감 => 정보손실=> 과거에는 사용했지만 최근에 사용하지 않는 이유
    학습 x => 원치않는 정보가 손실될 수 있다 => 최근에는 stride 보폭을 변경하면서 사이즈 조절

전체적 구조 40p
최근에는 컨볼루션 층만으로도 가능(보폭 변경)