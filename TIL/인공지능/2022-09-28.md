파이토치 oige 튜토리얼??


# 선형대수
## 벡터
### 벡터 간 거리 측정 방법
* 1차놈 
* 2차놈
12p
 왜 색있는 곳(최솟값)으로 가면 오버피팅?
옵티멀 값

어떤 규제(가중치제한)를 선택하는지에 따라 손실함수는 같은데 가중치가 다를 수 있다.
L1 sparse
L2 
즉 규제로써 크기(거리)를 이용할 수 있다

## 벡터의 곱셈
스칼라 곱셈, 벡터 내적, 벡터 외적(얘는 공부)
## 선형 결합
14p
벡터의 선형 연산(덧셈,곱셈) 을 통해 새로운 벡터를 얻는 연산=> 벡터 공간 형성
### 선형 독립, 선형 종속
ex>14p
선형 독립: 데이터는 고차원 => 중복되지 않은 데이터를 표현하는 최소의 벡터=> 차원을 낮추고 특징은 그대로 보유하고 있는 최소의 벡터(공간을 표현할 때 서로 독립인 벡터 사용) ex>PCA (최근 TSNE)

## 공간과 기저
기저: 공간을 생성하는 일종의 뼈대=> 즉 공간을 생성할 때 최소한 필요한 선형 독립 벡터 집합
차원은 기저 벡터의 개수를 의미
랭크: 벡터들의 조합인 행렬에서 선형 독립인 행 혹은 열의 개수를 의미(연립 방정식)

## 행렬의 곱셈
유사도 측정과 비슷함
### 행렬 곱셈의 물리적 의미
    선형 변환과 함수의 이해
    즉 공간을 변형함으로서 특징을 찾음

## 분해
벡터를 분해하면 더 많은 정보를 확인할 수도 있다
### 고유값 분해
고유 값 : 변한 크기
고유 벡터 : 선형변환을 했을때 방향은 변하지 않고 크기만 변하는 벡터(기저는 방향이 안 변하기 때문, 즉 기저를 찾는 과정)
21 22p
즉 고유벡터는 기준점으로 선정할 수 있다.(이 축에다가 고차원의 정보를 집어넣으면 차원은 줄어드는데 특징은 그대로??)
### 고유 분해
23p
정사각형일때만 사용가능
### 행렬 특잇값 분해
24p
정사각형이 아닐때 사용
#### 이와 같은 분해를 하는 이유:
25p 행렬분해의 기하학적 해석
    역행렬 구하기
    차원 축소 기준점 찾기

## 역행렬의 원리
심층학습에서는 사용x=>추천시스템(거대 행렬의 분해=> 행렬 백트라이제이션?), 연립방정식 해 찾기
즉 일반적인 데이터 과학에서 쓰임
역행렬의 필수 조건18p
## 행렬식
심층학습 x 데이터과학에서 쓰임

## 정부호 행렬
심층학습 x 공간의 볼록,오목 파악

# 확률 및 통계
27p
기계학습이 처리할 데이터에 내재된 `불확실성`을 다루기 위해 확률과 통계에 기초를 둠
## 확률
확률변수: 사건에 따라 값이 결정되는 변수
## 분포
확률질량함수: 이산(PMF)
확률밀도함수: 연속(PDF)
## 확률벡터
확률변수의 조합
결합분포: 두개 이상의 확률변수를 동시에 고려한 확률 분포