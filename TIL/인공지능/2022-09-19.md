# 기계학습개요
41p
신경망의 러닝 알고리즘 : gradient algorism
함수를 개선할떄 신경망의 경사를 이용해서 개선

42p
학습 알고리즘
    최적의 매개변수를 찾는 행위 : 손실함수(J(세타))를 제일 작게만드는 세타값을 찾자
                                    러닝알고리즘의 목적
    매개변수의 변화를 반복하여 최소의 목적함수를 만족하는 최적의 해를 찾아가는 수치적 방법
    의사 알고리즘
        w에 따라 손실함수가 달라진다
        w?? 손실함수와 매개변수 사이의 관계
        w값을 변경(미분값의 정보를 이용해서)하면서 error값을 변경하면서 미분의 반대방향으로 갱신(사진)
        매개변수 -> 예측값 -> 손실함수
        손실함수를 줄이고자 손실함수의 미분값을 이용해서 매개변수를 변경하겠다.
기계학습의 개발절차
    소수의 데이터수집- 간단한 모델 생성- 데이터 대량 수집 - 모델 수정 - 데이터 수집
    BUT 선형모델로는 한계가 많다.
        1. 비선형모델로 확장한다
        2. 데이터공간의 전환 -> 사고의 전환- 원 특징공간의 변환 필요
            45P 예시 representation matter???
            직교좌표계-> 원통좌표계
            특징공간
표현학습 representation learning
    모델 스스로가 특징공간을 찾아준다
    표현의 변환-> 공간의변화(원래 공간보다 과업에 유리하게 변경)-> 특징 추출-> 추상화      
이 과정을 신경망을 통해 계층적으로 점진적 표현 학습 => 심층학습 딥러닝
    representation learning하고의 차이 
        신경망의 은닉층을 통해서 반복적으로 하는가 안하는가

# 과대적합과 과소적합
매개변수 > 자유도 > 모델용량
과소적합 : 모델의 용량이 작아서 오차가 큰 현상
과대적합 : 모델의 큰 용량으로 인해서 학습과정에서 불필요한 노이즈를 학습하거나 훈련 데이터집합에만 과대몰입하여 학습하낟

(딴말 : 도메인일반화?)

## 편향과 변동
    목적값에 벗어나있는 정도(편향)
    모델의 결과가 변하는 정도(변동)
        ex> 2차 편향 큼 변동 작음, 12차 편향 작음 변동 큼
    일반화 성능(편향오차+변동오차)

기계학습의 목표: 일반화 성능 보장(Eout=0)
    Ein: 훈련데이터 속의 에러, Eout: 일반에서의 에러 
    Eout를 알 수 없기 때문에 Ein을 통해서 Eout를 예측한다
    학습 타당성(learning Feasibility)
        Q1> 일반화 : Eout=Ein 인지
        Q2> 근사화 : Ein=0 인지
    Q1: (변동)작은 용량 모델, 충분한 훈련 데이터 집합(훈련데이터의 확보) 
        일반화에 실패시 과대적합이기 때문에 규제`20220921` 필요
    Q2: (편향)큰 용량 모델, 충분한 훈련 보장(여러번의 반복) 
        근사화에 실패시 과소적합이기 때문에 향상된 최적화 방법을 적용

    처음에 큰 용량모델을 선택함으로서 근사화를 만족하고
    데이터 양을 충분히 함과 규제를 통해서 오버피팅 대처하고 일반화를 만족시키려고 노력한다


