# 대표적인 합성곱 신경망
## LeNet
* imagenet 사용 x
* 특징 추출: CONV-POOL-CONV-POOL-CONV의 다섯 층
* 분류: 1개의 완전 연결층 (FC - softmax)
## AlexNet
* 합성곱 층 5개 + 완전 연결 층(Fully connected layer) 3개
* 각 층마다 290400-186624-64896-43264-4096-4096-1000개의 매개변수 가짐
    * 합성곱 층은 200만개, 완전연결 층은 6500만개 가량의 매개변수: 완전연결 층에 30배 많은 매개변수
        * 향후 합성곱 신경망의 구조는 완전연결 층의 매개변수를 줄이는 방향으로 발전

### AlexNet 학습 성공 요인
* 외적 요인
    * imagenet
    * GPGPU 병렬처리(당시 GPU 메모리 크기제한으로 인해 GPU를 분할해서 학습)
        * 각각의 GPU에 layer를 쪼갠 것처럼 작용
        * 중간에 (사진의 3번째, maxpooling) 학습결과를 섞어준다
        * 49p
* 내적 요인
    * ReLU 활성함수 사용 (Gradient vanishing 방지)
    * 지역 반응 정규화 기법 적용(최근에는 사용x => 더 좋은 정규화가 나와서)
        * 한 특징이 크면 다른 특징이 사라지는 현상 방지(큰 특징 낮추고 낮은 특징 키우기)
    * 과잉적합 방지하는 여러 규제 기법 적용
        * 데이터 확대 (한 개의 데이터를 잘라내기, 반전으로 확대(but 원래 가지고있는 특징은 유지해야함))
        * 드롭아웃 (완전연결층에서 사용)
            * 모든 파라미터 연결 시 불필요한 가중치도 학습가능(overfitting)
                * 이를 해결하기 위해 필요한 파라미터들만 학습시키기 위해 드롭아웃 적용
                    * 드롭아웃: 확률적으로 파라미터끼리의 연결을 끊는다
                        * 전방 계산 할 때 50퍼의 파라미터만 살림 이후 가중치 갱신도 남아있는 파라미터의 가중치만 갱신됨
                        * 학습할때만 사용, 실제 test할 때는 완전연결로 복원한다=> 기댓값이 변한다
    * 추론단계에서 앙상블 적용 (최근에는 사용x)
        * test할 때 데이터증대를 한 후 증대된 데이터의 예측 평균으로 최종 인식

## VGGNet
* 3*3의 작은 필터 사용 -> 신경망을 더 깊게 만든다
### 작은 필터의 이점
* 52p
    * 큰 필터보다 매개변수의 수가 줄어든다 and 신경망이 깊어진다
    * 행과 열을 분해할 시 매개변수의 수가 줄어든다 => 모델의 경량화
        * 모바일넷?
        * 성능이 줄어든다
### 1*1 합성곱 필터
* VGG에서 가능성을 실험했지만 GoogLeNet에서 사용됨
* 차원 통합(출력 depth=1), 차원 축소(출력 depth=k)
    * 출력(특징맵)의 depth를 필터의 수로 결정 가능 => depth를 통일시키거나 연산량을 감소시킬 수 있다

* 54p 전역평균요약:
    * 특징맵의 depth를 출력노드의 길이로 바꾼 후 1장의 특징맵의 평균값을 결과값에 mapping (여기서 결과값에 맵핑되는 특징맵은 결과의 특징을 추상화할 가능성이 높다)

## GoogLeNet
* 인셉션 모듈
    * NIN
        * multilayer perceptron => fully connected?
        * 이 multilayer perceptron중 필요한 부분만을 선택할 수 있다
    * 하나의 layer 층에서 여러 크기의 conv(filter)를 적용할 수 있다
        * stride를 통해서 크기 맞춘다
        * depth가 커지는 것을 1*1를 먼저 걸어서 depth 정보를 조절한다

    * 55p
* 인셉션 모듈 9개 완전연결층 1개(global 머시기 때문에=> 매개변수 수 감소)
* 보조 분류기
    * ReLU를 쓰더라도 너무 깊어서 gradient가 소멸할 수 있다
    * 중간 마다 분류기를 설치해서 gradient가 소멸될 시 여기서 보충한다
